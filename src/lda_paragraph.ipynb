{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a58cd6b-2df5-4a69-9fd6-e94687f2d1e7",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170f9d4-9396-4424-a081-df397896c755",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74f49e1-26fc-4ca5-9173-b2e48d856a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# LDA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ec22c8-7308-44b5-aafb-6576f472210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cindy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load NLTK tools\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c60439-8811-4865-b998-f75923b7f8ce",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19505a90-366e-4f71-bd91-db4042bcf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcripts\n",
    "data_folder = '../data/'\n",
    "df = pd.read_csv(data_folder + 'scraped_data_no_names.csv')\n",
    "transcript_df = pd.DataFrame(df['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd6e4e4-e2dc-405c-843d-52e0386f2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraphs\n",
    "paragraph_df = pd.read_csv(data_folder + 'paragraphs_dataset.csv')\n",
    "paragraph_df.dropna(inplace=True) # Remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549d581-dc88-4a75-bab1-7e4435c0388f",
   "metadata": {},
   "source": [
    "## Text Processing: NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748336a6-af01-4c80-b5dc-1ee884045b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_processing(df, column_name):\n",
    "    # Tokenization\n",
    "    df['tokenized'] = df[column_name].apply(tokenizer.tokenize)\n",
    "    \n",
    "    # Removing stopwords and Casefolding\n",
    "    df['no_stopwords'] = df['tokenized'].apply(\n",
    "        lambda l: [s.casefold() for s in l if s.casefold() not in stop_words and s not in stop_words])\n",
    "    \n",
    "    # Lemmatization\n",
    "    df['lemmatized'] = df['no_stopwords'].apply(\n",
    "        lambda l: [lemmatizer.lemmatize(s) for s in l])\n",
    "    \n",
    "    # # Removing custom stopwords\n",
    "    # custom_stop_words = {'000', 'b', 'k', '25', '29'}\n",
    "    # df['final'] = df['lemmatized'].apply(\n",
    "    #     lambda l: [s for s in l if s not in custom_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359a36a7-27c0-4d10-be7a-fae0b4db46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processing(transcript_df, 'transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61226f44-859a-40de-a79c-a79d815dda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processing(paragraph_df, 'paragraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a4e01-d06b-4fa9-bfac-2b652712c109",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0fb53-8b70-44c2-b671-555d87c825eb",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd8b0e8-b996-4efc-a8ca-98b9435c7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim dictionary\n",
    "dictionary = Dictionary(transcript_df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70785cb3-bd49-44cb-bf6d-c6b3b33ee30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in less than 30 documents and more than 50% documents, keep only the first 100000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below=30, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef19943-d6fe-4e78-bac4-55eee8f46949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(text) for text in transcript_df['lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d645b-e5ce-4024-bfbc-e6fab5faf8ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f9c43-161e-4dc0-8d2b-00fd16b3dfbc",
   "metadata": {},
   "source": [
    "## Topic Modelling: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e86cf-0300-46d3-874f-72a2c5f78c85",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10142ef4-0681-4b9a-92d7-e2ba386ea45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 2. Coherence score: 0.37923667672558947\n",
      "Number of topics: 3. Coherence score: 0.4473216941747626\n",
      "Number of topics: 4. Coherence score: 0.46385772490314464\n",
      "Number of topics: 5. Coherence score: 0.4341760000865092\n",
      "Number of topics: 6. Coherence score: 0.48432344950920414\n",
      "Number of topics: 7. Coherence score: 0.5397399082777165\n",
      "Number of topics: 8. Coherence score: 0.5112633546266613\n",
      "Number of topics: 9. Coherence score: 0.5058716022974443\n",
      "Number of topics: 10. Coherence score: 0.530195506167002\n",
      "Number of topics: 11. Coherence score: 0.5210125550025098\n"
     ]
    }
   ],
   "source": [
    "# Define range of number of topics\n",
    "num_topics_range = range(2, 12)\n",
    "\n",
    "# Compute coherence scores for different number of topics\n",
    "coherence_scores = []\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)\n",
    "    \n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=transcript_df['lemmatized'], corpus=corpus, coherence='c_v', topn=20)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    print(f\"Number of topics: {num_topics}. Coherence score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0a477e-913d-4d67-ae17-238935570df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model on the corpus\n",
    "num_topics = num_topics_range[np.array(coherence_scores).argmax()]\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327e2798-0d93-4847-ba51-e9cb9024a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# Visualize the results using pyLDAvis\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis_data, f'../results/lda_{num_topics}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7f9ba-8bd0-4387-9f36-9b468bc13ac8",
   "metadata": {},
   "source": [
    "### Get the top topic for each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d33c7e0-447f-4006-a2e5-8be4c01339f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topic(document, lda_model):\n",
    "    # Create a bag-of-words representation\n",
    "    bow = lda_model.id2word.doc2bow(document)\n",
    "    \n",
    "    # Get the topic distribution for the document\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    \n",
    "    # Sort the topics by probability (in descending order)\n",
    "    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the topic with the highest probability\n",
    "    return sorted_topics[0][0] + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f6fcb9d-9e94-45a0-8ff6-ac333c9263cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df['topic'] = paragraph_df['lemmatized'].apply(lambda x: get_document_topic(x, lda_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a3ebea-eb5b-4a62-865e-1168858e75d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>This is Wednesday, March 20.</td>\n",
       "      <td>[This, is, Wednesday, March, 20]</td>\n",
       "      <td>[wednesday, march, 20]</td>\n",
       "      <td>[wednesday, march, 20]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>March 20.</td>\n",
       "      <td>[March, 20]</td>\n",
       "      <td>[march, 20]</td>\n",
       "      <td>[march, 20]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>2019. I’m Cindy Kelly, and I’m in Pasadena, C...</td>\n",
       "      <td>[2019, I, m, Cindy, Kelly, and, I, m, in, Pasa...</td>\n",
       "      <td>[2019, cindy, kelly, pasadena, california, ing...</td>\n",
       "      <td>[2019, cindy, kelly, pasadena, california, ing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Inge-Juliana Sackmann Christy. And should I s...</td>\n",
       "      <td>[Inge, Juliana, Sackmann, Christy, And, should...</td>\n",
       "      <td>[inge, juliana, sackmann, christy, spell]</td>\n",
       "      <td>[inge, juliana, sackmann, christy, spell]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Yes, please.</td>\n",
       "      <td>[Yes, please]</td>\n",
       "      <td>[yes, please]</td>\n",
       "      <td>[yes, please]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>I-n-g-e, Juliana, J-u-l-i-a-n-a, Sackmann, S ...</td>\n",
       "      <td>[I, n, g, e, Juliana, J, u, l, i, a, n, a, Sac...</td>\n",
       "      <td>[n, g, e, juliana, j, u, l, n, sackmann, like,...</td>\n",
       "      <td>[n, g, e, juliana, j, u, l, n, sackmann, like,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>But I want to start with you. I want you to t...</td>\n",
       "      <td>[But, I, want, to, start, with, you, I, want, ...</td>\n",
       "      <td>[want, start, want, tell, us, born, became, in...</td>\n",
       "      <td>[want, start, want, tell, u, born, became, int...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Well, that’s a long story. My family backgrou...</td>\n",
       "      <td>[Well, that, s, a, long, story, My, family, ba...</td>\n",
       "      <td>[well, long, story, family, background, german...</td>\n",
       "      <td>[well, long, story, family, background, german...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>They were offered something by the Russian Cza...</td>\n",
       "      <td>[They, were, offered, something, by, the, Russ...</td>\n",
       "      <td>[offered, something, russian, czar, could, go,...</td>\n",
       "      <td>[offered, something, russian, czar, could, go,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Napoleon had just gone through much of Europe,...</td>\n",
       "      <td>[Napoleon, had, just, gone, through, much, of,...</td>\n",
       "      <td>[napoleon, gone, much, europe, invaded, much, ...</td>\n",
       "      <td>[napoleon, gone, much, europe, invaded, much, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name  \\\n",
       "0  Inge-Juliana Sackmann Christy   \n",
       "1  Inge-Juliana Sackmann Christy   \n",
       "2  Inge-Juliana Sackmann Christy   \n",
       "3  Inge-Juliana Sackmann Christy   \n",
       "4  Inge-Juliana Sackmann Christy   \n",
       "5  Inge-Juliana Sackmann Christy   \n",
       "6  Inge-Juliana Sackmann Christy   \n",
       "7  Inge-Juliana Sackmann Christy   \n",
       "8  Inge-Juliana Sackmann Christy   \n",
       "9  Inge-Juliana Sackmann Christy   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0                       This is Wednesday, March 20.   \n",
       "1                                          March 20.   \n",
       "2   2019. I’m Cindy Kelly, and I’m in Pasadena, C...   \n",
       "3   Inge-Juliana Sackmann Christy. And should I s...   \n",
       "4                                       Yes, please.   \n",
       "5   I-n-g-e, Juliana, J-u-l-i-a-n-a, Sackmann, S ...   \n",
       "6   But I want to start with you. I want you to t...   \n",
       "7   Well, that’s a long story. My family backgrou...   \n",
       "8  They were offered something by the Russian Cza...   \n",
       "9  Napoleon had just gone through much of Europe,...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                   [This, is, Wednesday, March, 20]   \n",
       "1                                        [March, 20]   \n",
       "2  [2019, I, m, Cindy, Kelly, and, I, m, in, Pasa...   \n",
       "3  [Inge, Juliana, Sackmann, Christy, And, should...   \n",
       "4                                      [Yes, please]   \n",
       "5  [I, n, g, e, Juliana, J, u, l, i, a, n, a, Sac...   \n",
       "6  [But, I, want, to, start, with, you, I, want, ...   \n",
       "7  [Well, that, s, a, long, story, My, family, ba...   \n",
       "8  [They, were, offered, something, by, the, Russ...   \n",
       "9  [Napoleon, had, just, gone, through, much, of,...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0                             [wednesday, march, 20]   \n",
       "1                                        [march, 20]   \n",
       "2  [2019, cindy, kelly, pasadena, california, ing...   \n",
       "3          [inge, juliana, sackmann, christy, spell]   \n",
       "4                                      [yes, please]   \n",
       "5  [n, g, e, juliana, j, u, l, n, sackmann, like,...   \n",
       "6  [want, start, want, tell, us, born, became, in...   \n",
       "7  [well, long, story, family, background, german...   \n",
       "8  [offered, something, russian, czar, could, go,...   \n",
       "9  [napoleon, gone, much, europe, invaded, much, ...   \n",
       "\n",
       "                                          lemmatized  topic  \n",
       "0                             [wednesday, march, 20]      1  \n",
       "1                                        [march, 20]      1  \n",
       "2  [2019, cindy, kelly, pasadena, california, ing...      1  \n",
       "3          [inge, juliana, sackmann, christy, spell]      1  \n",
       "4                                      [yes, please]      1  \n",
       "5  [n, g, e, juliana, j, u, l, n, sackmann, like,...      1  \n",
       "6  [want, start, want, tell, u, born, became, int...      6  \n",
       "7  [well, long, story, family, background, german...      1  \n",
       "8  [offered, something, russian, czar, could, go,...      1  \n",
       "9  [napoleon, gone, much, europe, invaded, much, ...      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a61b61-67cf-4926-8771-33d0f9d5a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph_df[['name', 'paragraph', 'topic']].to_csv(data_folder + 'paragraph_topic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d85233e0-30c0-4f99-9c1e-c91343e683bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.389489\n",
       "6    0.152956\n",
       "2    0.114355\n",
       "5    0.105793\n",
       "7    0.082253\n",
       "3    0.078198\n",
       "4    0.076955\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "paragraph_df['topic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c823c-d2b7-48e9-8897-65b0007a0a32",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
