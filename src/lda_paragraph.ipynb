{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a58cd6b-2df5-4a69-9fd6-e94687f2d1e7",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170f9d4-9396-4424-a081-df397896c755",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74f49e1-26fc-4ca5-9173-b2e48d856a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# LDA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ec22c8-7308-44b5-aafb-6576f472210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cindy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load NLTK tools\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c60439-8811-4865-b998-f75923b7f8ce",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19505a90-366e-4f71-bd91-db4042bcf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcripts\n",
    "data_folder = '../data/'\n",
    "df = pd.read_csv(data_folder + 'scraped_data_no_names.csv')\n",
    "transcript_df = pd.DataFrame(df['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd6e4e4-e2dc-405c-843d-52e0386f2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraphs\n",
    "paragraph_df = pd.read_csv(data_folder + 'paragraphs_dataset.csv')\n",
    "paragraph_df.dropna(inplace=True) # Remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549d581-dc88-4a75-bab1-7e4435c0388f",
   "metadata": {},
   "source": [
    "## Text Processing: NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748336a6-af01-4c80-b5dc-1ee884045b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_processing(df, column_name):\n",
    "    # Tokenization\n",
    "    df['tokenized'] = df[column_name].apply(tokenizer.tokenize)\n",
    "    \n",
    "    # Removing stopwords and Casefolding\n",
    "    df['no_stopwords'] = df['tokenized'].apply(\n",
    "        lambda l: [s.casefold() for s in l if s.casefold() not in stop_words and s not in stop_words])\n",
    "    \n",
    "    # Lemmatization\n",
    "    df['lemmatized'] = df['no_stopwords'].apply(\n",
    "        lambda l: [lemmatizer.lemmatize(s) for s in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359a36a7-27c0-4d10-be7a-fae0b4db46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processing(transcript_df, 'transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61226f44-859a-40de-a79c-a79d815dda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processing(paragraph_df, 'paragraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a4e01-d06b-4fa9-bfac-2b652712c109",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0fb53-8b70-44c2-b671-555d87c825eb",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd8b0e8-b996-4efc-a8ca-98b9435c7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim dictionary\n",
    "dictionary = Dictionary(transcript_df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70785cb3-bd49-44cb-bf6d-c6b3b33ee30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in less than 30 documents and more than 50% documents, keep only the first 100000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below=30, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef19943-d6fe-4e78-bac4-55eee8f46949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(text) for text in transcript_df['lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d645b-e5ce-4024-bfbc-e6fab5faf8ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f9c43-161e-4dc0-8d2b-00fd16b3dfbc",
   "metadata": {},
   "source": [
    "## Topic Modelling: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e86cf-0300-46d3-874f-72a2c5f78c85",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10142ef4-0681-4b9a-92d7-e2ba386ea45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 2. Coherence score: 0.37923667672558947\n",
      "Number of topics: 3. Coherence score: 0.4473216941747626\n",
      "Number of topics: 4. Coherence score: 0.46385772490314464\n",
      "Number of topics: 5. Coherence score: 0.4341760000865092\n",
      "Number of topics: 6. Coherence score: 0.48432344950920414\n",
      "Number of topics: 7. Coherence score: 0.5397399082777165\n",
      "Number of topics: 8. Coherence score: 0.5112633546266613\n",
      "Number of topics: 9. Coherence score: 0.5058716022974443\n",
      "Number of topics: 10. Coherence score: 0.530195506167002\n",
      "Number of topics: 11. Coherence score: 0.5210125550025098\n"
     ]
    }
   ],
   "source": [
    "# Define range of number of topics\n",
    "num_topics_range = range(2, 12)\n",
    "\n",
    "# Compute coherence scores for different number of topics\n",
    "coherence_scores = []\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)\n",
    "    \n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=transcript_df['lemmatized'], corpus=corpus, coherence='c_v', topn=20)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    print(f\"Number of topics: {num_topics}. Coherence score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0a477e-913d-4d67-ae17-238935570df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model on the corpus\n",
    "num_topics = num_topics_range[np.array(coherence_scores).argmax()]\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327e2798-0d93-4847-ba51-e9cb9024a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# Visualize the results using pyLDAvis\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis_data, f'../results/lda_{num_topics}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9456e-9556-4495-8b5a-a7d2bf8abdb6",
   "metadata": {},
   "source": [
    "Following several qualitative and quantitative evaluation, we choose to train three LDA models with 7, 30 and 90 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e64b505-2250-4abb-b9f1-294d6932ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_7 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=7, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc87df9-bfd0-41c1-989b-a5ce91e322dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_30 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=30, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3544dc06-017e-4784-925f-8dc9f0b8330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_90 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=90, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7f9ba-8bd0-4387-9f36-9b468bc13ac8",
   "metadata": {},
   "source": [
    "### Application od the LDA models on the paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ed83c1-d37e-4bfe-94af-564f03e8a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topic_info(document, lda_model, num_words=20):\n",
    "    # Create a bag-of-words representation\n",
    "    bow = lda_model.id2word.doc2bow(document)\n",
    "    \n",
    "    # Get the topic distribution for the document\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    \n",
    "    # Sort the topics by their probability in descending order\n",
    "    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N words for the most probable topic\n",
    "    top_topic = 0\n",
    "    top_topic_words = []\n",
    "    top_topic_contribution = 0.0\n",
    "\n",
    "    if len(sorted_topics) > 0:\n",
    "        top_topic_id = sorted_topics[0][0]\n",
    "        top_topic = lda_model.print_topic(top_topic_id)\n",
    "        top_topic_contribution = sorted_topics[0][1]\n",
    "        topic_words = lda_model.show_topic(top_topic_id, num_words)\n",
    "        top_topic_words = [word for word, _ in topic_words]\n",
    "        \n",
    "    top_topic_id += 1\n",
    "\n",
    "    return top_topic_id, top_topic_words, top_topic_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09951d0c-381a-484a-967a-ff19822c1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['topic_7', 'words_topic_7', 'contribution_topic_7']] = paragraph_df['lemmatized'].apply(\n",
    "    lambda x: pd.Series(get_document_topic_info(x, lda_model_7, num_words=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4b8f370-3a0f-4481-b156-51f365a7fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['topic_30', 'words_topic_30', 'contribution_topic_30']] = paragraph_df['lemmatized'].apply(\n",
    "    lambda x: pd.Series(get_document_topic_info(x, lda_model_30, num_words=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c842f0c-1a43-4919-b009-a86c59d5c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['topic_90', 'words_topic_90', 'contribution_topic_90']] = paragraph_df['lemmatized'].apply(\n",
    "    lambda x: pd.Series(get_document_topic_info(x, lda_model_90, num_words=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f91ff44a-f006-44a6-8623-5f72e7a64a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df.rename(columns={'name': 'interviewee'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2a3ebea-eb5b-4a62-865e-1168858e75d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interviewee</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>words_topic_7</th>\n",
       "      <th>contribution_topic_7</th>\n",
       "      <th>topic_30</th>\n",
       "      <th>words_topic_30</th>\n",
       "      <th>contribution_topic_30</th>\n",
       "      <th>topic_90</th>\n",
       "      <th>words_topic_90</th>\n",
       "      <th>contribution_topic_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>This is Wednesday, March 20.</td>\n",
       "      <td>[This, is, Wednesday, March, 20]</td>\n",
       "      <td>[wednesday, march, 20]</td>\n",
       "      <td>[wednesday, march, 20]</td>\n",
       "      <td>1</td>\n",
       "      <td>[father, mother, child, santa, married, fe, to...</td>\n",
       "      <td>0.913163</td>\n",
       "      <td>10</td>\n",
       "      <td>[chemistry, barrier, berkeley, plutonium, colu...</td>\n",
       "      <td>0.812533</td>\n",
       "      <td>85</td>\n",
       "      <td>[unit, lunch, hall, mess, chemistry, box, ohio...</td>\n",
       "      <td>0.775456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>March 20.</td>\n",
       "      <td>[March, 20]</td>\n",
       "      <td>[march, 20]</td>\n",
       "      <td>[march, 20]</td>\n",
       "      <td>1</td>\n",
       "      <td>[father, mother, child, santa, married, fe, to...</td>\n",
       "      <td>0.876739</td>\n",
       "      <td>10</td>\n",
       "      <td>[chemistry, barrier, berkeley, plutonium, colu...</td>\n",
       "      <td>0.743623</td>\n",
       "      <td>10</td>\n",
       "      <td>[berkeley, caltech, lawrence, barrier, feynman...</td>\n",
       "      <td>0.697666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>2019. I’m Cindy Kelly, and I’m in Pasadena, C...</td>\n",
       "      <td>[2019, I, m, Cindy, Kelly, and, I, m, in, Pasa...</td>\n",
       "      <td>[2019, cindy, kelly, pasadena, california, ing...</td>\n",
       "      <td>[2019, cindy, kelly, pasadena, california, ing...</td>\n",
       "      <td>1</td>\n",
       "      <td>[father, mother, child, santa, married, fe, to...</td>\n",
       "      <td>0.932992</td>\n",
       "      <td>9</td>\n",
       "      <td>[santa, fe, dorothy, hotel, mexico, bus, broth...</td>\n",
       "      <td>0.852029</td>\n",
       "      <td>1</td>\n",
       "      <td>[mother, happy, father, child, indian, husband...</td>\n",
       "      <td>0.821683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Inge-Juliana Sackmann Christy. And should I s...</td>\n",
       "      <td>[Inge, Juliana, Sackmann, Christy, And, should...</td>\n",
       "      <td>[inge, juliana, sackmann, christy, spell]</td>\n",
       "      <td>[inge, juliana, sackmann, christy, spell]</td>\n",
       "      <td>1</td>\n",
       "      <td>[father, mother, child, santa, married, fe, to...</td>\n",
       "      <td>0.787578</td>\n",
       "      <td>13</td>\n",
       "      <td>[father, mother, child, dad, kid, sister, pare...</td>\n",
       "      <td>0.605069</td>\n",
       "      <td>13</td>\n",
       "      <td>[father, mother, child, dad, sister, parent, m...</td>\n",
       "      <td>0.024054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Yes, please.</td>\n",
       "      <td>[Yes, please]</td>\n",
       "      <td>[yes, please]</td>\n",
       "      <td>[yes, please]</td>\n",
       "      <td>1</td>\n",
       "      <td>[father, mother, child, santa, married, fe, to...</td>\n",
       "      <td>0.787563</td>\n",
       "      <td>27</td>\n",
       "      <td>[girl, bus, married, road, food, dance, badge,...</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>13</td>\n",
       "      <td>[father, mother, child, dad, sister, parent, m...</td>\n",
       "      <td>0.024090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     interviewee  \\\n",
       "0  Inge-Juliana Sackmann Christy   \n",
       "1  Inge-Juliana Sackmann Christy   \n",
       "2  Inge-Juliana Sackmann Christy   \n",
       "3  Inge-Juliana Sackmann Christy   \n",
       "4  Inge-Juliana Sackmann Christy   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0                       This is Wednesday, March 20.   \n",
       "1                                          March 20.   \n",
       "2   2019. I’m Cindy Kelly, and I’m in Pasadena, C...   \n",
       "3   Inge-Juliana Sackmann Christy. And should I s...   \n",
       "4                                       Yes, please.   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                   [This, is, Wednesday, March, 20]   \n",
       "1                                        [March, 20]   \n",
       "2  [2019, I, m, Cindy, Kelly, and, I, m, in, Pasa...   \n",
       "3  [Inge, Juliana, Sackmann, Christy, And, should...   \n",
       "4                                      [Yes, please]   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0                             [wednesday, march, 20]   \n",
       "1                                        [march, 20]   \n",
       "2  [2019, cindy, kelly, pasadena, california, ing...   \n",
       "3          [inge, juliana, sackmann, christy, spell]   \n",
       "4                                      [yes, please]   \n",
       "\n",
       "                                          lemmatized  topic_7  \\\n",
       "0                             [wednesday, march, 20]        1   \n",
       "1                                        [march, 20]        1   \n",
       "2  [2019, cindy, kelly, pasadena, california, ing...        1   \n",
       "3          [inge, juliana, sackmann, christy, spell]        1   \n",
       "4                                      [yes, please]        1   \n",
       "\n",
       "                                       words_topic_7  contribution_topic_7  \\\n",
       "0  [father, mother, child, santa, married, fe, to...              0.913163   \n",
       "1  [father, mother, child, santa, married, fe, to...              0.876739   \n",
       "2  [father, mother, child, santa, married, fe, to...              0.932992   \n",
       "3  [father, mother, child, santa, married, fe, to...              0.787578   \n",
       "4  [father, mother, child, santa, married, fe, to...              0.787563   \n",
       "\n",
       "   topic_30                                     words_topic_30  \\\n",
       "0        10  [chemistry, barrier, berkeley, plutonium, colu...   \n",
       "1        10  [chemistry, barrier, berkeley, plutonium, colu...   \n",
       "2         9  [santa, fe, dorothy, hotel, mexico, bus, broth...   \n",
       "3        13  [father, mother, child, dad, kid, sister, pare...   \n",
       "4        27  [girl, bus, married, road, food, dance, badge,...   \n",
       "\n",
       "   contribution_topic_30  topic_90  \\\n",
       "0               0.812533        85   \n",
       "1               0.743623        10   \n",
       "2               0.852029         1   \n",
       "3               0.605069        13   \n",
       "4               0.600599        13   \n",
       "\n",
       "                                      words_topic_90  contribution_topic_90  \n",
       "0  [unit, lunch, hall, mess, chemistry, box, ohio...               0.775456  \n",
       "1  [berkeley, caltech, lawrence, barrier, feynman...               0.697666  \n",
       "2  [mother, happy, father, child, indian, husband...               0.821683  \n",
       "3  [father, mother, child, dad, sister, parent, m...               0.024054  \n",
       "4  [father, mother, child, dad, sister, parent, m...               0.024090  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7a61b61-67cf-4926-8771-33d0f9d5a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['interviewee', 'paragraph', 'topic_7', 'words_topic_7', 'contribution_topic_7', 'topic_30', 'words_topic_30', 'contribution_topic_30', 'topic_90', 'words_topic_90', 'contribution_topic_90']].to_csv(data_folder + 'paragraph_topic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c823c-d2b7-48e9-8897-65b0007a0a32",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
