{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a58cd6b-2df5-4a69-9fd6-e94687f2d1e7",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170f9d4-9396-4424-a081-df397896c755",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74f49e1-26fc-4ca5-9173-b2e48d856a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# LDA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ec22c8-7308-44b5-aafb-6576f472210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cindy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load NLTK tools\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c60439-8811-4865-b998-f75923b7f8ce",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19505a90-366e-4f71-bd91-db4042bcf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcripts\n",
    "data_folder = '../data/'\n",
    "df = pd.read_csv(data_folder + 'scraped_data_no_names.csv')\n",
    "transcript_df = pd.DataFrame(df['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd6e4e4-e2dc-405c-843d-52e0386f2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraphs\n",
    "paragraph_df = pd.read_csv(data_folder + 'paragraphs_dataset.csv')\n",
    "paragraph_df.dropna(inplace=True) # Remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549d581-dc88-4a75-bab1-7e4435c0388f",
   "metadata": {},
   "source": [
    "## Text Processing: NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748336a6-af01-4c80-b5dc-1ee884045b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_processing(df, column_name):\n",
    "    # Tokenization\n",
    "    df['tokenized'] = df[column_name].apply(tokenizer.tokenize)\n",
    "    \n",
    "    # Removing stopwords and Casefolding\n",
    "    df['no_stopwords'] = df['tokenized'].apply(\n",
    "        lambda l: [s.casefold() for s in l if s.casefold() not in stop_words and s not in stop_words])\n",
    "    \n",
    "    # Lemmatization\n",
    "    df['lemmatized'] = df['no_stopwords'].apply(\n",
    "        lambda l: [lemmatizer.lemmatize(s) for s in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359a36a7-27c0-4d10-be7a-fae0b4db46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processing(transcript_df, 'transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61226f44-859a-40de-a79c-a79d815dda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_processing(paragraph_df, 'paragraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a4e01-d06b-4fa9-bfac-2b652712c109",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0fb53-8b70-44c2-b671-555d87c825eb",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd8b0e8-b996-4efc-a8ca-98b9435c7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gensim dictionary\n",
    "dictionary = Dictionary(transcript_df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70785cb3-bd49-44cb-bf6d-c6b3b33ee30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in less than 30 documents and more than 50% documents, keep only the first 100000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below=30, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ef19943-d6fe-4e78-bac4-55eee8f46949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(text) for text in transcript_df['lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d645b-e5ce-4024-bfbc-e6fab5faf8ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f9c43-161e-4dc0-8d2b-00fd16b3dfbc",
   "metadata": {},
   "source": [
    "## Topic Modelling: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e86cf-0300-46d3-874f-72a2c5f78c85",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10142ef4-0681-4b9a-92d7-e2ba386ea45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 2. Coherence score: 0.37923667672558947\n",
      "Number of topics: 3. Coherence score: 0.4473216941747626\n",
      "Number of topics: 4. Coherence score: 0.46385772490314464\n",
      "Number of topics: 5. Coherence score: 0.4341760000865092\n",
      "Number of topics: 6. Coherence score: 0.48432344950920414\n",
      "Number of topics: 7. Coherence score: 0.5397399082777165\n",
      "Number of topics: 8. Coherence score: 0.5112633546266613\n",
      "Number of topics: 9. Coherence score: 0.5058716022974443\n",
      "Number of topics: 10. Coherence score: 0.530195506167002\n",
      "Number of topics: 11. Coherence score: 0.5210125550025098\n"
     ]
    }
   ],
   "source": [
    "# Define range of number of topics\n",
    "num_topics_range = range(2, 12)\n",
    "\n",
    "# Compute coherence scores for different number of topics\n",
    "coherence_scores = []\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)\n",
    "    \n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=transcript_df['lemmatized'], corpus=corpus, coherence='c_v', topn=20)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append(coherence_score)\n",
    "    print(f\"Number of topics: {num_topics}. Coherence score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0a477e-913d-4d67-ae17-238935570df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model on the corpus\n",
    "num_topics = num_topics_range[np.array(coherence_scores).argmax()]\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327e2798-0d93-4847-ba51-e9cb9024a0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cindy/opt/anaconda3/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# Visualize the results using pyLDAvis\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis_data, f'../results/lda_{num_topics}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9456e-9556-4495-8b5a-a7d2bf8abdb6",
   "metadata": {},
   "source": [
    "Following several qualitative and quantitative evaluation, we choose to train three LDA models with 7, 30 and 90 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e64b505-2250-4abb-b9f1-294d6932ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_7 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=7, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc87df9-bfd0-41c1-989b-a5ce91e322dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_30 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=30, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3544dc06-017e-4784-925f-8dc9f0b8330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_90 = LdaModel(corpus=corpus, id2word=dictionary, num_topics=90, alpha='auto', eta='auto', passes=10, iterations=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "514decdb-6b32-40d8-94ef-670c412e49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe storing the top 20 words for each topic of these models\n",
    "models = {\n",
    "    7: lda_model_7,\n",
    "    30: lda_model_30,\n",
    "    90: lda_model_90\n",
    "}\n",
    "\n",
    "lda_words = pd.DataFrame(columns=['model', 'topic'] + [f'word_{i}' for i in range(1, 21)])\n",
    "\n",
    "for model_idx, model in models.items():\n",
    "    # Get the number of topics for the current model\n",
    "    num_topics = model.num_topics\n",
    "\n",
    "    for topic in range(num_topics):\n",
    "        # Get the top words for the current topic and model\n",
    "        topic_words = model.show_topic(topic, 20)\n",
    "        word_list = [word for word, _ in topic_words]\n",
    "\n",
    "        # Create a row for the current topic and model\n",
    "        row = [f'lda_model_{model_idx}', topic+1] + word_list\n",
    "\n",
    "        # Append the row to the DataFrame\n",
    "        lda_words.loc[len(lda_words)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2354128-c1fa-433b-ace3-aaf6d29d3842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>topic</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>...</th>\n",
       "      <th>word_11</th>\n",
       "      <th>word_12</th>\n",
       "      <th>word_13</th>\n",
       "      <th>word_14</th>\n",
       "      <th>word_15</th>\n",
       "      <th>word_16</th>\n",
       "      <th>word_17</th>\n",
       "      <th>word_18</th>\n",
       "      <th>word_19</th>\n",
       "      <th>word_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lda_model_7</td>\n",
       "      <td>1</td>\n",
       "      <td>father</td>\n",
       "      <td>mother</td>\n",
       "      <td>child</td>\n",
       "      <td>santa</td>\n",
       "      <td>married</td>\n",
       "      <td>fe</td>\n",
       "      <td>town</td>\n",
       "      <td>dad</td>\n",
       "      <td>...</td>\n",
       "      <td>kid</td>\n",
       "      <td>husband</td>\n",
       "      <td>bus</td>\n",
       "      <td>road</td>\n",
       "      <td>barrack</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>camp</td>\n",
       "      <td>hall</td>\n",
       "      <td>sister</td>\n",
       "      <td>hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lda_model_7</td>\n",
       "      <td>2</td>\n",
       "      <td>grove</td>\n",
       "      <td>barrier</td>\n",
       "      <td>engineering</td>\n",
       "      <td>equipment</td>\n",
       "      <td>diffusion</td>\n",
       "      <td>metal</td>\n",
       "      <td>operation</td>\n",
       "      <td>president</td>\n",
       "      <td>...</td>\n",
       "      <td>committee</td>\n",
       "      <td>decision</td>\n",
       "      <td>construction</td>\n",
       "      <td>york</td>\n",
       "      <td>dupont</td>\n",
       "      <td>keith</td>\n",
       "      <td>lawrence</td>\n",
       "      <td>chemical</td>\n",
       "      <td>columbia</td>\n",
       "      <td>colonel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lda_model_7</td>\n",
       "      <td>3</td>\n",
       "      <td>fermi</td>\n",
       "      <td>szilard</td>\n",
       "      <td>reactor</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>plutonium</td>\n",
       "      <td>student</td>\n",
       "      <td>wigner</td>\n",
       "      <td>experiment</td>\n",
       "      <td>...</td>\n",
       "      <td>science</td>\n",
       "      <td>professor</td>\n",
       "      <td>neutron</td>\n",
       "      <td>chain</td>\n",
       "      <td>enrico</td>\n",
       "      <td>graduate</td>\n",
       "      <td>hanford</td>\n",
       "      <td>dupont</td>\n",
       "      <td>dr</td>\n",
       "      <td>chemist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lda_model_7</td>\n",
       "      <td>4</td>\n",
       "      <td>hanford</td>\n",
       "      <td>richland</td>\n",
       "      <td>river</td>\n",
       "      <td>dupont</td>\n",
       "      <td>waste</td>\n",
       "      <td>construction</td>\n",
       "      <td>000</td>\n",
       "      <td>reactor</td>\n",
       "      <td>...</td>\n",
       "      <td>town</td>\n",
       "      <td>tank</td>\n",
       "      <td>columbia</td>\n",
       "      <td>00</td>\n",
       "      <td>study</td>\n",
       "      <td>worker</td>\n",
       "      <td>report</td>\n",
       "      <td>cleanup</td>\n",
       "      <td>camp</td>\n",
       "      <td>law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lda_model_7</td>\n",
       "      <td>5</td>\n",
       "      <td>reactor</td>\n",
       "      <td>plutonium</td>\n",
       "      <td>radiation</td>\n",
       "      <td>fuel</td>\n",
       "      <td>power</td>\n",
       "      <td>hanford</td>\n",
       "      <td>design</td>\n",
       "      <td>neutron</td>\n",
       "      <td>...</td>\n",
       "      <td>facility</td>\n",
       "      <td>dupont</td>\n",
       "      <td>system</td>\n",
       "      <td>tube</td>\n",
       "      <td>radioactive</td>\n",
       "      <td>operation</td>\n",
       "      <td>separation</td>\n",
       "      <td>level</td>\n",
       "      <td>engineering</td>\n",
       "      <td>rod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>lda_model_90</td>\n",
       "      <td>86</td>\n",
       "      <td>barrier</td>\n",
       "      <td>grove</td>\n",
       "      <td>reactor</td>\n",
       "      <td>columbia</td>\n",
       "      <td>dunning</td>\n",
       "      <td>kellex</td>\n",
       "      <td>carbide</td>\n",
       "      <td>mr</td>\n",
       "      <td>...</td>\n",
       "      <td>keith</td>\n",
       "      <td>urey</td>\n",
       "      <td>york</td>\n",
       "      <td>diffusion</td>\n",
       "      <td>c</td>\n",
       "      <td>baker</td>\n",
       "      <td>plastic</td>\n",
       "      <td>power</td>\n",
       "      <td>graphite</td>\n",
       "      <td>production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>lda_model_90</td>\n",
       "      <td>87</td>\n",
       "      <td>teller</td>\n",
       "      <td>refugee</td>\n",
       "      <td>jew</td>\n",
       "      <td>plutonium</td>\n",
       "      <td>helium</td>\n",
       "      <td>edward</td>\n",
       "      <td>000</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>neutron</td>\n",
       "      <td>explosive</td>\n",
       "      <td>weapon</td>\n",
       "      <td>jewish</td>\n",
       "      <td>proton</td>\n",
       "      <td>temperature</td>\n",
       "      <td>camp</td>\n",
       "      <td>germany</td>\n",
       "      <td>begin</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>lda_model_90</td>\n",
       "      <td>88</td>\n",
       "      <td>dupont</td>\n",
       "      <td>design</td>\n",
       "      <td>construction</td>\n",
       "      <td>engineering</td>\n",
       "      <td>equipment</td>\n",
       "      <td>hanford</td>\n",
       "      <td>chemical</td>\n",
       "      <td>cell</td>\n",
       "      <td>...</td>\n",
       "      <td>crane</td>\n",
       "      <td>explosive</td>\n",
       "      <td>operation</td>\n",
       "      <td>foot</td>\n",
       "      <td>facility</td>\n",
       "      <td>powder</td>\n",
       "      <td>division</td>\n",
       "      <td>concrete</td>\n",
       "      <td>steel</td>\n",
       "      <td>safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>lda_model_90</td>\n",
       "      <td>89</td>\n",
       "      <td>engine</td>\n",
       "      <td>division</td>\n",
       "      <td>trailer</td>\n",
       "      <td>computer</td>\n",
       "      <td>camp</td>\n",
       "      <td>brother</td>\n",
       "      <td>husband</td>\n",
       "      <td>chuckle</td>\n",
       "      <td>...</td>\n",
       "      <td>crew</td>\n",
       "      <td>inaudible</td>\n",
       "      <td>kid</td>\n",
       "      <td>card</td>\n",
       "      <td>barrack</td>\n",
       "      <td>bridge</td>\n",
       "      <td>clean</td>\n",
       "      <td>horse</td>\n",
       "      <td>construction</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>lda_model_90</td>\n",
       "      <td>90</td>\n",
       "      <td>japanese</td>\n",
       "      <td>japan</td>\n",
       "      <td>hiroshima</td>\n",
       "      <td>surrender</td>\n",
       "      <td>truman</td>\n",
       "      <td>president</td>\n",
       "      <td>decision</td>\n",
       "      <td>emperor</td>\n",
       "      <td>...</td>\n",
       "      <td>survivor</td>\n",
       "      <td>peace</td>\n",
       "      <td>nagasaki</td>\n",
       "      <td>bombing</td>\n",
       "      <td>1945</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>000</td>\n",
       "      <td>ii</td>\n",
       "      <td>prisoner</td>\n",
       "      <td>thousand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  topic    word_1     word_2        word_3       word_4  \\\n",
       "0     lda_model_7      1    father     mother         child        santa   \n",
       "1     lda_model_7      2     grove    barrier   engineering    equipment   \n",
       "2     lda_model_7      3     fermi    szilard       reactor    chemistry   \n",
       "3     lda_model_7      4   hanford   richland         river       dupont   \n",
       "4     lda_model_7      5   reactor  plutonium     radiation         fuel   \n",
       "..            ...    ...       ...        ...           ...          ...   \n",
       "122  lda_model_90     86   barrier      grove       reactor     columbia   \n",
       "123  lda_model_90     87    teller    refugee           jew    plutonium   \n",
       "124  lda_model_90     88    dupont     design  construction  engineering   \n",
       "125  lda_model_90     89    engine   division       trailer     computer   \n",
       "126  lda_model_90     90  japanese      japan     hiroshima    surrender   \n",
       "\n",
       "        word_5        word_6     word_7      word_8  ...    word_11  \\\n",
       "0      married            fe       town         dad  ...        kid   \n",
       "1    diffusion         metal  operation   president  ...  committee   \n",
       "2    plutonium       student     wigner  experiment  ...    science   \n",
       "3        waste  construction        000     reactor  ...       town   \n",
       "4        power       hanford     design     neutron  ...   facility   \n",
       "..         ...           ...        ...         ...  ...        ...   \n",
       "122    dunning        kellex    carbide          mr  ...      keith   \n",
       "123     helium        edward        000           3  ...    neutron   \n",
       "124  equipment       hanford   chemical        cell  ...      crane   \n",
       "125       camp       brother    husband     chuckle  ...       crew   \n",
       "126     truman     president   decision     emperor  ...   survivor   \n",
       "\n",
       "       word_12       word_13    word_14      word_15      word_16     word_17  \\\n",
       "0      husband           bus       road      barrack    wonderful        camp   \n",
       "1     decision  construction       york       dupont        keith    lawrence   \n",
       "2    professor       neutron      chain       enrico     graduate     hanford   \n",
       "3         tank      columbia         00        study       worker      report   \n",
       "4       dupont        system       tube  radioactive    operation  separation   \n",
       "..         ...           ...        ...          ...          ...         ...   \n",
       "122       urey          york  diffusion            c        baker     plastic   \n",
       "123  explosive        weapon     jewish       proton  temperature        camp   \n",
       "124  explosive     operation       foot     facility       powder    division   \n",
       "125  inaudible           kid       card      barrack       bridge       clean   \n",
       "126      peace      nagasaki    bombing         1945  grandfather         000   \n",
       "\n",
       "      word_18       word_19     word_20  \n",
       "0        hall        sister        hill  \n",
       "1    chemical      columbia     colonel  \n",
       "2      dupont            dr     chemist  \n",
       "3     cleanup          camp         law  \n",
       "4       level   engineering         rod  \n",
       "..        ...           ...         ...  \n",
       "122     power      graphite  production  \n",
       "123   germany         begin         low  \n",
       "124  concrete         steel      safety  \n",
       "125     horse  construction         cut  \n",
       "126        ii      prisoner    thousand  \n",
       "\n",
       "[127 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc994c3c-dcdb-4c3f-9f36-45cd17f782dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_words.to_csv('../results/lda_top20_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7f9ba-8bd0-4387-9f36-9b468bc13ac8",
   "metadata": {},
   "source": [
    "### Application od the LDA models on the paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f06abb7d-4e51-493a-a3e0-0bcd81d53bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topic(document, lda_model):\n",
    "    # Create a bag-of-words representation\n",
    "    bow = lda_model.id2word.doc2bow(document)\n",
    "    \n",
    "    # Get the topic distribution for the document\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "\n",
    "    # Sort the topics by their probability in descending order\n",
    "    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top topic ID and its contribution\n",
    "    top_topic_id = None\n",
    "    top_topic_contribution = 0.0\n",
    "\n",
    "    if len(sorted_topics) > 0:\n",
    "        top_topic_id = sorted_topics[0][0]\n",
    "        top_topic_contribution = sorted_topics[0][1]\n",
    "        \n",
    "    # Correct the index\n",
    "    top_topic_id += 1\n",
    "\n",
    "    return int(top_topic_id), top_topic_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09951d0c-381a-484a-967a-ff19822c1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['topic_7', 'contribution_topic_7']] = paragraph_df['lemmatized'].apply(\n",
    "    lambda x: pd.Series(get_document_topic(x, lda_model_7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4b8f370-3a0f-4481-b156-51f365a7fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['topic_30', 'contribution_topic_30']] = paragraph_df['lemmatized'].apply(\n",
    "    lambda x: pd.Series(get_document_topic(x, lda_model_30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c842f0c-1a43-4919-b009-a86c59d5c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['topic_90', 'contribution_topic_90']] = paragraph_df['lemmatized'].apply(\n",
    "    lambda x: pd.Series(get_document_topic(x, lda_model_90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f91ff44a-f006-44a6-8623-5f72e7a64a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df.rename(columns={'name': 'interviewee'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84eb7e62-dea0-4662-9df2-b2f5eda45e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert topic ids to integer\n",
    "paragraph_df[['topic_7', 'topic_30', 'topic_90']] = paragraph_df[['topic_7', 'topic_30', 'topic_90']].apply(pd.to_numeric, downcast='integer', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2a3ebea-eb5b-4a62-865e-1168858e75d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interviewee</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>contribution_topic_7</th>\n",
       "      <th>topic_30</th>\n",
       "      <th>contribution_topic_30</th>\n",
       "      <th>topic_90</th>\n",
       "      <th>contribution_topic_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>This is Wednesday, March 20.</td>\n",
       "      <td>[This, is, Wednesday, March, 20]</td>\n",
       "      <td>[wednesday, march, 20]</td>\n",
       "      <td>[wednesday, march, 20]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913163</td>\n",
       "      <td>10</td>\n",
       "      <td>0.812532</td>\n",
       "      <td>10</td>\n",
       "      <td>0.775581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>March 20.</td>\n",
       "      <td>[March, 20]</td>\n",
       "      <td>[march, 20]</td>\n",
       "      <td>[march, 20]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876739</td>\n",
       "      <td>10</td>\n",
       "      <td>0.743623</td>\n",
       "      <td>10</td>\n",
       "      <td>0.697666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>2019. I’m Cindy Kelly, and I’m in Pasadena, C...</td>\n",
       "      <td>[2019, I, m, Cindy, Kelly, and, I, m, in, Pasa...</td>\n",
       "      <td>[2019, cindy, kelly, pasadena, california, ing...</td>\n",
       "      <td>[2019, cindy, kelly, pasadena, california, ing...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932992</td>\n",
       "      <td>9</td>\n",
       "      <td>0.852029</td>\n",
       "      <td>17</td>\n",
       "      <td>0.821436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Inge-Juliana Sackmann Christy. And should I s...</td>\n",
       "      <td>[Inge, Juliana, Sackmann, Christy, And, should...</td>\n",
       "      <td>[inge, juliana, sackmann, christy, spell]</td>\n",
       "      <td>[inge, juliana, sackmann, christy, spell]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.787578</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596854</td>\n",
       "      <td>13</td>\n",
       "      <td>0.024090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inge-Juliana Sackmann Christy</td>\n",
       "      <td>Yes, please.</td>\n",
       "      <td>[Yes, please]</td>\n",
       "      <td>[yes, please]</td>\n",
       "      <td>[yes, please]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.787563</td>\n",
       "      <td>27</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>27</td>\n",
       "      <td>0.537536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     interviewee  \\\n",
       "0  Inge-Juliana Sackmann Christy   \n",
       "1  Inge-Juliana Sackmann Christy   \n",
       "2  Inge-Juliana Sackmann Christy   \n",
       "3  Inge-Juliana Sackmann Christy   \n",
       "4  Inge-Juliana Sackmann Christy   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0                       This is Wednesday, March 20.   \n",
       "1                                          March 20.   \n",
       "2   2019. I’m Cindy Kelly, and I’m in Pasadena, C...   \n",
       "3   Inge-Juliana Sackmann Christy. And should I s...   \n",
       "4                                       Yes, please.   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                   [This, is, Wednesday, March, 20]   \n",
       "1                                        [March, 20]   \n",
       "2  [2019, I, m, Cindy, Kelly, and, I, m, in, Pasa...   \n",
       "3  [Inge, Juliana, Sackmann, Christy, And, should...   \n",
       "4                                      [Yes, please]   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0                             [wednesday, march, 20]   \n",
       "1                                        [march, 20]   \n",
       "2  [2019, cindy, kelly, pasadena, california, ing...   \n",
       "3          [inge, juliana, sackmann, christy, spell]   \n",
       "4                                      [yes, please]   \n",
       "\n",
       "                                          lemmatized  topic_7  \\\n",
       "0                             [wednesday, march, 20]        1   \n",
       "1                                        [march, 20]        1   \n",
       "2  [2019, cindy, kelly, pasadena, california, ing...        1   \n",
       "3          [inge, juliana, sackmann, christy, spell]        1   \n",
       "4                                      [yes, please]        1   \n",
       "\n",
       "   contribution_topic_7  topic_30  contribution_topic_30  topic_90  \\\n",
       "0              0.913163        10               0.812532        10   \n",
       "1              0.876739        10               0.743623        10   \n",
       "2              0.932992         9               0.852029        17   \n",
       "3              0.787578         1               0.596854        13   \n",
       "4              0.787563        27               0.600599        27   \n",
       "\n",
       "   contribution_topic_90  \n",
       "0               0.775581  \n",
       "1               0.697666  \n",
       "2               0.821436  \n",
       "3               0.024090  \n",
       "4               0.537536  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7a61b61-67cf-4926-8771-33d0f9d5a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[['interviewee', 'paragraph', 'topic_7', 'contribution_topic_7', 'topic_30', 'contribution_topic_30', 'topic_90', 'contribution_topic_90']].to_csv(data_folder + 'paragraph_topic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c823c-d2b7-48e9-8897-65b0007a0a32",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
